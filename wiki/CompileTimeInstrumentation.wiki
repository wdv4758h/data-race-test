#summary LLVM compile-time instrumentation for TSan support

We've implemented an experimental compile-time instrumentation pass for LLVM that allows us to directly link ThreadSanitizer against the client code instead of using binary instrumentation frameworks.

An instrumented program collects the events (memory accesses, routine calls/exits) and passes it to the runtime library that serves as a proxy between the client code and ThreadSanitizer. The RTL also provides wrappers for functions that ThreadSanitizer should know about (synchronization primitives, allocation routines, [DynamicAnnotations dynamic annotations]

Once the LLVM prototype is finished, the runtime library could be used with other compilers, e.g. gcc.

= Pros and Cons =
Possible advantages:
  * no binary translation and instrumentation overhead at runtime
  * at compile time we know the semantic information that allows us to ignore accesses to thread-local memory (e.g. unescaped stack allocations)

Possible drawbacks:
  * we'll need to recompile libc and other system libraries or somehow hardcode the knowledge about their functions in order to find races on the memory passed to library functions

= Usage and current status =
See the code in http://code.google.com/p/data-race-test/source/browse/#svn/trunk/llvm

Before using the tool you should build the opt plugin and the runtime library:
{{{
cd llvm
make
}}}

The prerequisites necessary for building ThreadSanitizer.so are listed in `llvm/opt/README`

To compile a small test containing one file:

`$ llvm/scripts/instrument_and_link.sh [x86|x86_64] path/to/program.c` -- produces `path/to/program` binary with builtin ThreadSanitizer

To build a larger program with `make`, use the wrappers for `gcc` and `g++`.
For example, this is how racecheck_unittest is built:
{{{
TSAN_IGNORE=racecheck_unittest.ignore make OPT=1 OMIT_DYNAMIC_ANNOTATIONS_IMPL=1 OMIT_CPP0X=1 PATH="`pwd`/../llvm/scripts/:$PATH" EXTRA_BUILD_SUFFIX=-llvm l64
}}}

To run the tests, just execute the resulting binary. Additional flags for ThreadSanitizer can be supplied via the TSAN_ARGS env variable.

*Note*: at the moment racecheck_unittest doesn't pass when linked against ThreadSanitizer.

= How it is done =
The wrappers for `gcc` and `g++` translate C/C++ code into LLVM assembly code using `llvm-gcc`. It is then instrumented using `opt`, which invokes `ThreadSanitizer.so` as a plugin. The instrumented code contains calls to the functions provided by the runtime library. If `gcc`/`g++` is used to link an executable binary, it is additionally linked with the ThreadSanitizer runtime (`tsan_rtl32.a` or `tsan_rtl64.a` depending on the word length)

== Instrumenting the client code ==
Each client module is instrumented independently, no link-time instrumentation is performed. For each module the following steps are done:
  * external declarations are inserted for the functions and global variables provided by the RTL
  * trace (superblock) instrumentation is done
  * each function call/exit is instrumented
  * debug info is written into the `tsan_rtl_debug_info` section

At the moment there are three external functions used by the client code. Those are: `bb_flush()` (used for flushing the thread-local event buffers, see below), `rtl_memcpy()` and `rtl_memmove()` (used to replace the [http://llvm.org/docs/LangRef.html#int_memcpy llvm.memcpy.*] and [http://llvm.org/docs/LangRef.html#int_memmove llvm.memmove.*] intrinsics).

During the instrumentation each function is divided into traces (also known as superblocks), which are pieces of the call graph with a single entry and (possibly) multiple exits containing no cycles. For each trace the maximum possible number of memory operations is known at compile time, so it is possible to create a static record containing the following information about the trace:
{{{
struct TraceInfoPOD {
  enum { kLiteRaceNumTids = 8 };  // takes zero bytes
  size_t n_mops_;
  size_t pc_;
  size_t counter_;
  uint32_t literace_counters[kLiteRaceNumTids];
  int32_t literace_num_to_skip[kLiteRaceNumTids];
  MopInfo mops_[1];
};
}}}
, where `MopInfo` is defined as follows:
{{{
struct MopInfo {
  uintptr_t pc;
  uint32_t  size;
  bool      is_write;
};
}}}

Each trace entry is instrumented with the call to `bb_flush(&current_trace_info)` (typically 2 instructions on x64), which flushes the events collected executing the previous trace and tells ThreadSanitizer where to get the information about the current trace. Each memory operation ([http://llvm.org/docs/LangRef.html#i_load load] or [http://llvm.org/docs/LangRef.html#i_store store]) is instrumented with an LLVM `store` instruction that puts the memory address being accessed into a thread-local buffer (at offset known at compile time). So the following code:
{{{
  store %dyn_value, %dyn_address
}}}

is transformed into:
{{{
  getelementptr @TLEB, i32 0, i64 <static_tleb_offset>
  store %dyn_address, %tleb_address
  store %dyn_value, %dyn_address
}}}

If the` --workaround-vptr-race` flag is set to true, the first trace of each object's destructor is instrumented in a way that replaces the memory address with NULL if and only if the memory operation is a store and it rewrites the current memory contents with the same value. For example, for following code:
{{{
  store %new, %a
}}}
the %ptr to be put into TLEB is calculated as follows:
{{{
  %old      = load %a
  %destcast = ptrtoint %a to i32
  %neq      = icmp ne %old, %new
  %neqcast  = zext %neq to i32
  %intptr   = mul %destcast, %neqcast
  %ptr      = inttoptr %intptr to i32
}}}

After all traces are instrumented each function is furnished with a prologue and an epilogue that maintain the ThreadSanitizer shadow stack. The shadow stack is provided by the RTL and is a thread-local variable of the following type:
{{{
const size_t kMaxCallStackSize = 1 << 12;
struct CallStackPod {
  uintptr_t *end_;
  uintptr_t pcs_[kMaxCallStackSize];
};
}}}

Upon each function entry the following instructions are executed:
{{{
  %end_ptr = getelementptr %1* @ShadowStack, i64 0, i32 0
  %end = load i64** %0
  store i64 %addr, i64* %end
  %new_end = getelementptr i64* %end, i32 1
  store i64* %new_end, i64** %end_ptr
}}}

, effectively pushing the function address into the stack:
{{{
  *ShadowStack.end_ = (uintptr_t)addr;
  ShadowStack.end_++;
}}}

Before leaving the function the original stack top is restored:
{{{
  store i64* %end, i64** %end_ptr
}}}

The client code instrumentation is done on the SSA level, so the real symbol addresses are unknown at instrumentation time. To solve this problem each memory operation and each function call are assigned synthetic program counter values that depend on the current memory operations and call counters. The assumption is that the number of memory operations and function calls within a function is always less then the size of that function in bytes. Unfortunately this is not always right because of tail-call optimizations. The synthetic addresses are stored along with their location in the source code (path, filename, line) in the `tsan_rtl_debug_info` section of the binary, which is read by ThreadSanitizer at runtime to provide the exact debug info.


= Useful ThreadSanitizer flags =
  * `--suppressions=<filename>`
  * `--ignore=<filename>`
  * `--literace_sampling`
  * `--v`

= Building and testing Chromium =
== Building Chromium ==
  * set up a client and check out the source code following the instruction at http://code.google.com/p/chromium/wiki/LinuxBuildInstructions
  * patch the client with the patch from http://codereview.chromium.org/6524008 , which contains the necessary changes:
    * the default implementation of dynamic_annotations is not linked with Chromium, because the RTL already provides its own
    * the OVERRIDE macro is expanded to no-op, because llvm-gcc doesn't seem to support it
    * the overridden versions of system allocation functions are disabled (we want to use our wrappers, not those from Chromium)
    * .gyp/include.gypi has some valuable GYP settings
  * run `gclient runhooks`:
{{{
HOME=`pwd` gclient runhooks
}}}
  * build `yasm` using `gcc`. Due to some problems the yasm binary doesn't work if built with `llvm-gcc`:
{{{{
make BUILDTYPE=Release out/Release/yasm
}}}}
  * build Chromium:
{{{
export SCRIPT_PATH=/path/to/data-race-test/llvm/scripts/
TSAN_IGNORE=`pwd`/tools/valgrind/tsan/ignores.txt make -j16 -e CXX="$SCRIPT_PATH/g++" CC="$SCRIPT_PATH/gcc" PATH="$SCRIPT_PATH:$PATH" BUILDTYPE=Release out/Release/chrome
}}}

= Future development =
== Short term ==
  * inline the [ LiteRaceSampling literace sampling] counters -- *DONE*
  * make racecheck_unittest pass (see http://code.google.com/p/data-race-test/issues/detail?id=52 for the status)
  * try this on something big (Chromium?)
  * compare the performance with Valgrind- or PIN-based ThreadSanitizer
  * use ignore files during compilation -- *DONE*
  * document the items marked as done
  * use LLVM debug info to get better stack traces for inlined function calls (store several nested symbols for a single pc)

== Long term ==
  * make it possible to switch between instrumented and uninstrumented versions at runtime
  * use escape analysis to reduce the number of instrumented operations
  * use PIN or other lightweight instrumentation framework to handle uninstrumented libraries
  * address the possible unwinding issues brought by exception handling
  * write a gcc plugin for program instrumentation
  * implement *fast* event logging in the RTL for offline mode